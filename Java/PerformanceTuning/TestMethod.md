## 测试真实的应用

### 微基准测试
> 测量微小的代码单元性能
> - 调用同步方法的用时于非同步方法用时的比较
> - 创建线程的代价与使用线程池的代价
> - 执行某种算法的耗时和其替代实现的耗时

#### 原则
1. 必须使用被测试的结构。（如果没使用结果，可能会被编译器优化掉）
2. 不要包括无关的操作
3. 必须输入合理的参数
4. 需要`热身期`

### 宏基准测试
> 测量完整的整体程序的性能，包括它所使用的外部资源
> - 不进行整体的应用测试，就不可能知道那部分的优化会产生回报

### 介基准测试
> 一些实际工作，但不是完整的应用程序的基准测试


### 总结
1. 好的微基准测试即难写，价值也有限，如果必须使用它，可以用来快速了解性能，但不要依赖于它
2. 测试完整的应用是了解它实际运行的唯一途径
3. 在模块或者操作级别隔离性能，相对于全应用测试来说，是一种这种途径，但不是替代方法

## 多角度审视应用性能

### 批处理流逝时间
> 基于完成一批量工作所需要的时间

- 注意`JIT`对测试的影响
- 注意`缓存`对测试的影响

### 吞吐量
> 基于一段时间内所完成的工作量
> - 零思考时间

##### 常用指标
- 每秒事务数TPS
- 每秒请求数RPS
- 每秒操作次数OPS

### 响应时间
> 基于从客户端发送请求到响应之间的流逝时间
> - 有思考时间

#### 衡量方法
- 报告平均值
- 报告百分位值

## 用统计方法应对性能的变化

> 思考问题
> - 运行结果之间的差别，到底是因为性能有变化，还是因为测试的随机性（通过多次测试解决）

#### 统计方式
- [t检验](/Statistics/Method/Student's-t-test.md)

#### 总结
1. 正确判断测试结果间的差异需要统计分析，通过统计分析才能确定这些差异是不是归因于随机因素
2. 可以用严谨的[t检验](/Statistics/Method/Student's-t-test.md)来比较测试结果
3. [t检验](/Statistics/Method/Student's-t-test.md)可以告诉我们变动存在的概率，却无法告诉我们那种变动该忽略，哪一种该追查

## 尽早频繁测试
#### 方式
- 自动化一切
- 测试一切
> 需要收集能够想象到的一切数据
> - 运行过程中采集的系统信息：
> 	- CPU使用率
> 	- 磁盘使用率
> 	- 网络使用率
> 	- 内存使用率
> - 应用的日志
> 	- 应用产生的日志
> 	- 垃圾收集器的日志
> - JFR记录的信息
> - 对系统影响较小的信息分析（Profiling）信息
> - 周期性线程堆栈
> - 堆分析数据
> - 系统其他部分的数据
> 	- 数据库机器的系统统计
> 	- 所有数据库的诊断输出
> `使用场景`
> - CPU使用率上升，参考新能分析信息，弄清楚是什么花费了这么多时间
> - GC时间边长，查阅堆性能分析信息，弄清楚什么消耗了这么多内存
> - GC和CPU时间都减少了，某些地方的竞争可能降低了性能：
> 	- 栈数据可以指示特定的同步瓶颈
> 	- JFR记录可用来发现应用的延迟
> 	- 数据库日志可以发现数据库竞争加剧的线索
- 在真实的系统上运行

## [Next Java性能工具箱](./TestTool.md)
